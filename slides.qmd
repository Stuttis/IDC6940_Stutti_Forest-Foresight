---
title: "World of Random Forests and Articles of interest"
subtitle: "This is a Slide Test with Lit Review"
author: "Stutti Smit-Johnson (Advisor: Dr. Seals)"
date: '`r Sys.Date()`'
format:
  revealjs
css: style.css
course: Capstone Projects in Data Science
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---
<!--

::: callout-important
**Remember:** Your goal is to make your audience understand and care
about your findings. By crafting a compelling story, you can effectively
communicate the value of your data science project.

Carefully read this template since it has instructions and tips to
writing!

More information about `revealjs`:
<https://quarto.org/docs/reference/formats/presentations/revealjs.html>
:::

-->

## Lit Review 1

The book *Data Mining with Decision Trees: Theory and Applications* [@Horobet2023-bv] provides an in-depth look at decision trees, a popular tool in data mining for their simplicity, transparency, and efficiency. Decision trees are easy to follow, require minimal computational effort, and perform well on large datasets with various data types. The book explains how to prevent overfitting using pre-pruning, post-pruning, and cost-complexity pruning, and covers evaluation techniques like cross-validation and bootstrapping. It also discusses common algorithms (ID3, C4.5, CART) and the limitations of decision trees, such as their tendency to overlook attribute combinations. Overall, it’s a valuable guide for understanding decision trees and their applications.


## Lit Review 2

The paper "Financial Forecast in Business and an Application Proposal: The Case of Random Forest Technique" [@Orhan2023-hm] examines the effectiveness of the Random Forest (RF) algorithm in predicting financial performance. Using data from five companies listed on Borsa Istanbul (2009-2020), key financial variables were estimated with the RF technique. The model utilized 113 variables, including macroeconomic indicators like inflation and GDP growth, achieving a 90.9% accuracy rate. The study concludes that RF is a reliable tool for financial forecasting, especially when incorporating macroeconomic factors, but suggests further improvements by adding more external variables.

## Lit Review 3

Leo Breiman’s 2001 paper on Random Forests [@Breiman_undated-tp] introduces the algorithm as a classifier made up of multiple decision trees, each voting for the most popular class. It emphasizes Random Forests' ability to improve classification accuracy through tree diversity and explains that accuracy depends on the strength and independence of individual trees. Breiman highlights the advantages of Random Forests, including handling large, high-dimensional datasets, estimating variable importance, and managing missing data. The method works well for classification, less so for regression, and shows lower error rates on larger datasets. Different types of randomness can also enhance results.

## Lit Review 4

The paper "Random Forests for Classification in Ecology" [@Cutler2007-pe] explores the underutilized Random Forest (RF) method in ecological studies. RF works by creating multiple classification trees from bootstrap samples and combining their predictions, improving accuracy compared to single trees. It excels with ecological data, which is often high-dimensional, complex, and has many missing values. In the study, RF was tested on three ecological datasets—plant invasions, rare lichen, and cavity-nesting birds—and outperformed other methods (LDA, logistic regression, etc.), showing high accuracy. The paper advocates for broader adoption of RF in ecology.

## Lit Review 5

J.R. Quinlan's paper [@Quinlan1987-cz] addresses the complexity of decision trees in expert systems, proposing four methods for simplifying them: Cost-Complexity Pruning, Reduced Error Pruning, Pessimistic Pruning, and Simplifying to Production Rules. These methods reduce tree complexity by pruning irrelevant conditions. Tested across six domains (e.g., Hypothyroid diagnosis, Chess Endgame), all methods significantly reduced tree size, improving interpretability. For example, in the Hypothyroid domain, tree nodes were reduced from 23.6 to as low as 11.0, while the Production Rules method simplified the tree to just 3 rules. The study shows that simplifying decision trees improves clarity and utility in expert systems.


## Lit Review 6

The book "Data Mining with Decision Trees: Theory and Applications" [@Rokach_undated-sg] details the widespread use of decision trees in data mining due to their simplicity, ease of interpretation, and high predictive performance. Decision trees work by recursively partitioning data and are favored for their ability to handle large datasets and various data types. However, overly complex trees can become hard to interpret. The book discusses methods like pre-pruning, post-pruning, and cost-complexity pruning to prevent overfitting, and it explores algorithms like ID3, C4.5, and CART. It also highlights limitations, such as the nearsighted nature of decision tree induction algorithms. Overall, it's a valuable resource for understanding decision trees and their applications. 


<!--
## Introduction  {.smaller}

-   Develop a storyline that captures attention and maintains interest.

-   Your audience is your peers

-   Clearly state the problem or question you're addressing.

-   Introduce why it is relevant needs.

-   Provide an overview of your approach.

In kernel estimator, weight function is known as kernel function
[@efr2008]. Cite this paper [@bro2014principal]. The GEE [@wang2014].
The PCA [@daffertshofer2004pca]\*

## Methods

-   Detail the models or algorithms used.

-   Justify your choices based on the problem and data.



## Data Exploration and Visualization

-   Describe your data sources and collection process.

-   Present initial findings and insights through visualizations.

-   Highlight unexpected patterns or anomalies.

## Data Exploration and Visualization {.smaller}

A study was conducted to determine how...

```{r, warning=FALSE, echo=F, message=FALSE}
# loading packages 
library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
```

```{r, warning=FALSE, echo=F}
# Load Data
#kable(head(murders))

ggplot1 = murders %>% ggplot(mapping = aes(x=population/10^6, y=total)) 

  ggplot1 + geom_point(aes(col=region), size = 4) +
  geom_text_repel(aes(label=abb)) +
  scale_x_log10() +
  scale_y_log10() +
  geom_smooth(formula = "y~x", method=lm,se = F)+
  xlab("Populations in millions (log10 scale)") + 
  ylab("Total number of murders (log10 scale)") +
  ggtitle("US Gun Murders in 2010") +
  scale_color_discrete(name = "Region")+
      theme_bw()
  

```

## Modeling and Results

-   Explain your data preprocessing and cleaning steps.

-   Present your key findings in a clear and concise manner.

-   Use visuals to support your claims.

-   **Tell a story about what the data reveals.**

```{r}

```

## Conclusion

-   Summarize your key findings.

-   Discuss the implications of your results.

-->

## References
