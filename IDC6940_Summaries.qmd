---
title: "Random Forest Summaries"
subtitle: "Summarizing various articles found on Random Forest method and it's application"
author: "Stutti Smit-Johnson (Advisor: Dr. Seals)"
date: "2024-09-08" 
format:
  html:
    code-fold: true
course: Capstone Projects in Data Science
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---


*Summary 1*

The book ‘Data Mining with decision trees: theory and applications’ explains the concepts of decision trees in detail. Due to it’s simple technique in predicting and explaining relationships between measurements about an item and its target value, the use of Decision Trees is very popular and common in the world of data mining. There are several key features of decision trees that are advantageous. Decision Trees are self-explanatory, easy to follow, has relatively small computational effort and high predictive performance, useful for large datasets and is flexible in handling various types of data like nominal, numeric, and textual. Decision Trees classify a target as a recursive partition. It consists of nodes that form a “root” with no incoming edges.  Decision Trees are popular for their simplicity and transparency, therefore, if decision trees become too complicated, in other words have too many nodes, they become useless. For complex trees, other procedures should be developed to simplify interpretation. The book covers pre-pruning, post-pruning and cost-complexity pruning to prevent overfitting and improve generalization of decision trees. The book also examines cross-validation and bootstrapping as validation metrics for evaluating performance, and accuracy of decision trees. The book touches upon some common algorithms for decision tree induction such as ID3, C4.5, CART, CHAID, and QUEST. It also details the disadvantages of decision trees. One such disadvantage is the nearsighted nature of decision tree induction algorithms where inducers look only one level ahead. Such strategies prefer tests that score high in isolation and may overlook combinations of attributes. Using deeper lookahead strategies is considered computationally expensive and not proven useful. All in all, the book is a great guide for anyone that wants to understand decision trees in data mining and its application to practical use cases. 

Rokach, L., & Maimon, O. (2008). Data mining with decision trees: theory and applications. World Scientific.

*Summary 2*

The paper written by J.R. Quinlan discusses decision tree usage within expert/artificial intelligence systems. The problem discussed is that decision trees become too complex too quickly which makes them hard to understand and use in expert systems. Four methods to simplify decision trees discussed are as follows: Cost-Complexity Pruning, Reduced Error Pruning, Pessimistic Pruning and Simplifying to Production Rules.  Each of these methods simplifies the trees by using different pruning ways and removing irrelevant conditions. In order to test these simplified methods, data from six different domains were chosen: Hypothyroid diagnosis, Discordant assay, LEDDigits, Consumer credit applications, Chess Endgame, and Probabilistic classification over disjunctions. Average size of the trees is noted on each: before applying the simplifying methods and after. All pruning methods significantly reduced the complexity of the decision trees. For example, within the Hypothyroid domain, original tree had 23.6 nodes and after applying pruning they went down to between 11.0 and 14.4.  Production rules method achieved the most sizeable simplification – reducing to just 3.0 rules which makes it highly interpretable. The same was true with the Endgame domain. Original tree had 88.8 nodes. After applying the cost complexity pruning and production rules, it went down to 51.0 nodes and 11.6 rules respectively. Through the process of testing, the study was able to prove that simplifying decision trees does lead to better representation, is easier to understand and is useful in producing knowledge for expert systems. 


J.R. Quinlan (1987). Simplifying decision trees. , 27(3), 221–234. doi:10.1016/s0020-7373(87)80053-6


*Summary 3*

The paper ‘Random Forests for Classification in Ecology’ is an interesting one since it discusses the use of Random Forests method by ecologists and this method hasn’t been broadly used in this field yet. The Random Forests (RF) algorithm operates by fitting numerous classification trees to a data set and then combines their predictions. It begins by generating many bootstrap samples from the original data, with each sample containing about 63% of the data selected with replacement. The remaining data, which do not appear in these samples, are known as out-of-bag observations. For each bootstrap sample, a classification tree is constructed, but at each decision point within the tree, only a random subset of variables is considered for splitting the data. This randomness in variable selection helps to create diverse trees. Once all the trees are fully grown, they are used to predict the classes of the out-of-bag observations. The final prediction for each observation is made by taking a majority vote across all the trees, with ties being resolved randomly. This ensemble approach enhances the accuracy and stability of the predictions compared to individual trees. 
Random forests method can be particularly beneficial with ecological data since i) ecological data is often high dimensional with nonlinear and complex interactions among variables and ii) has many missing values among measured variables. Traditional statistical methods such as GLMs may lack in uncovering patterns and relationships we are seeking.  Three different ecological data sets were used in this study: invasive plant species, rare lichen species, and cavity-nesting birds. Using these data sets, the Random forests method was compared to four other classification methods – LDA, logistic regression, additive logistic regression, and classification trees.  Following predictions were made: the presence of invasive species in Lava Beds National Monument, presence of rare lichen species in the Pacific Northwest, and nest presence for cavity-nesting birds in Uinta Mountains, Utah. The random forest method showed high accuracy across all three data sets especially in identifying presences and identifying absences. The paper finally encourages the random forest method’s broader adoption in the field of ecology.

Cutler, D. R., Edwards, T. C. J., Beard, K. H., Cutler, A., Hess, K. T., Gibson, J., & Lawler, J. J. (2007). Random forests for classification in ecology. Ecology, 88(11), 2783–2792. https://doi.org/10.1890/07-0539.1


*Summary 4*

As stated in Leo Breiman’s paper on Random Forests, published in January of 2001: “A random forest is a classifier consisting of a collection of tree-structured classifiers {h(x,Θk ), k=1, ...} where the {Θk} are independent identically distributed random vectors and each tree casts a unit vote for the most popular class at input x. This paper describes the foundation and concepts of the random forest algorithm and theorizes its ability to improve classification accuracy by combining multiple decision trees. The paper uses Amit and Geman [1997] analysis to explain that the accuracy of random forests depends on the strength of the individual tree classifiers and a measure of the dependence between them. It states the various formulas of the algorithm that are responsible for characterizing accuracy of random forests. Additionally, the author discusses using random features for lowering generalization errors than other algorithms including the use of out-of-bag estimates to monitor error, strength and correlation. He discusses his experiments and results and the use of the algorithm with bagging and without. Furthermore, he discusses all the advantages of random forest method: handling large datasets with high dimentionality, providing estimates of variable importance, and dealing with missing data.

RANDOM FORESTS, January 2001 Leo Breiman Statistics Department, University of California Berkeley, CA 94720 https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf


*Summary 5*

The paper "Financial Forecast in Business and an Application Proposal: The Case of Random Forest Technique" explores the viability of the Random Forest (RF) algorithm in predicting future financial performance. The study uses data from five companies whose shares have been traded on Borsa Istanbul between 2009 and 2020. Financial statements (between 2009 and 2020) of these businesses were obtained from the Public Disclosure Platform website. Variables such as current & fixed assets, equity, revenue, and net income have been estimated by use of the random forest technique.  As stated in the paper, “Random Forest is frequently preferred in classification and regression analyses because it produces reliable results by using the average of more than one decision tree and allows working with any number of trees (Biau-Scornet,2016:198).”  By leveraging 113 variables, including macroeconomic indicators like inflation, exchange rates, and GDP growth, the RF model showed an overall forecasting accuracy of 90.9%.  The research concludes that Random Forest is an effective tool for financial forecasting in businesses, particularly when coupled with non-financial and macroeconomic factors. However, further enhancements, such as the inclusion of additional external variables, could improve its predictive power in volatile periods like 2020. The study promotes RF as a valuable model for decision-makers in financial planning and risk management, offering high reliability in predicting future financial outcomes.

İşletmelerde Finansal Kestirim Ve Bir Uygulama Önerisi: Rassal Orman Tekniği. By: ORHAN, Abdullah; SAĞLAM, Necdet. Journal of Accounting & Finance / Muhasebe ve Finansman Dergisi. Jul2023, Issue 99, p171-193. 23p. DOI: 10.25095/mufad.1254043


*Summary 6*

“The theoretical framework examining business performance originates in the paradigm of industrial organization developed during 1940-1950 by Bain and Mason (Porter, 1981) which was further improved by Porter (1979), Schmalensee (1985) and Rumelt (1991).”  Differences in business performance and competitive positions have been studied for decades. The paper titled "Foreign Versus Local Ownership and Performance in Eastern Versus Western EU: A Random Forest Application" discusses how foreign and local ownership influences business performance across the European Union using the Random Forest machine learning algorithm. The study explores differences between Eastern and Western EU firms in 27 industries from 9 sectors between 2009 and 2016. Variables studied are personnel costs, labor productivity, and gross investment. The model results show that personnel costs per employee are the most significant variable differentiating foreign from locally owned companies. The model was applied to 1,080 business units from the EU, each from a different sector and industry, but also region (Eastern or Western part of the EU), and operating under different ownership, i.e., foreign or locally-owned. The main objective of classifying economic activity within the EU is that the authors wanted to identify whether business performance brought on by foreign versus local ownership may be explained by headquarters’ location, industry of operation and a reduced set of other performance variables. It was concluded that locally owned companies have an edge against the foreign-owned ones in terms of the importance of their gross investments compared to turnover and of the ratio between value added and turnover in eight out of nine sectors. These results are a sign that there is a stronger propensity of locally-owned companies towards investments versus a weaker investment activity of foreign-owned companies.

Horobet, Alexandra; Popovici, Oana Cristina; Bulai, Vlad; Belascu, Lucian; Rosca, Eugen. Engineering Economics. 2023, Vol. 34 Issue 2, p123-138. 16p. DOI: 10.5755/j01.ee.34.2.29499.
