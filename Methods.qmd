---
title: "Power of Random Forest in Business"
subtitle: "Methodology"
author: "Stutti (Advisor: Dr. Seals)"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: Capstone Projects in Data Science
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---
# Decision trees – Random Forest

## Methods

There are several methods within the Random Forest algorithm to achieve accuracy and reduce overfitting. I decided to run Random Forest Regression method because it performs well with large datasets, de-emphasizes irrelevant features and avoid overfitting as compared to traditional decision tree contruction.

## Model Creation

Two regression Random Forest models were created to predict Total Price of Sales activity using several features from the SSI Sales dataset.

The TotalPrice variable was modeled using the randomForest() function in R. The Random Forest model was trained to predict Sales prices on orders based on the following predictors:

  **•	OPCO:** The distributor ordering from SSI for the end Client.
  
  **•	Product:** The product being sold.
  
  **•	Substrate:** The material type of the product being sold.
  
  **•	RequestedDeliveryDate:** The date the sale was requested to be delivered.
  
  **•	qtyOrdered:** The quantity of items ordered in the sales transaction.
  
  **•	UnitPrice:** The price per unit of each product.
  
  **•	Class:** The end Client on the sales transaction.

The first model was created using tree size of 100 (ntree = 100). The second, tree size of 300 (ntree = 300). Missing values were handled during model training and not excluded from final output for data consistency. Feature importance and proximity measure were set within the models for further analysis. 

Lastly, since the dataset is large, the data was split into smaller samples. The createDataPartition() function in R was used to split the data into training and test sets based on target variable TotalPrice and distribution of TotalPrice was maintained between the two sets. 80% of the data was used to train data and 20% was used to test. The training set was used to train the random forest models and helped it learn the relationships between the predictors and the target variable TotalPrice. The testing set was used to evaluate the models' performance. The number of variables were randomly sampled at each split (mtry = 3) using the default value for regression.

For visualization purposes, below is an example of a tree from the Random Forest for SSI Sales data.

```{r}
#chooseCRANmirror()

#install.packages("rpart", repos = "https://cloud.r-project.org/")
#install.packages("rattle", repos = "https://cloud.r-project.org/")
#install.packages("rpart")  # Recursive Partitioning and Regression Trees
#install.packages("rattle")  # used to plot/visualize the tree
#install.packages("partykit")  # used to plot/visualize the tree
library(rpart)
library(rpart.plot)
#install.packages("randomForest")
library(randomForest)
library(tidyverse)
library(dplyr)
library(ggplot2)
library(caret)

SaleData <- read_csv('/Users/ss/Documents/HR Misc/Masters/IDC6940 Capstone in Data Science/Data/DataSet1.csv') 

# List of records to exclude for shipping
exclude_products <- c("OUTBOUND SHIPPING", "Outbound Shipping", "Shipping Charge", "SHIPPING CHARGE") 

# Filter out returns/credits (Price <= 0) and shipping charges
SaleData1 <- SaleData %>%
  filter(UnitPrice > 0, TotalPrice > 0, !Product %in% exclude_products)

# Split the data into training and testing sets
set.seed(6079)
trainIndex <- createDataPartition(SaleData1$TotalPrice, p = 0.8, list = FALSE, times = 1)
trainData <- SaleData1[trainIndex, ]
testData  <- SaleData1[-trainIndex, ]


# Fit a Random Forest model to predict TotalPrice (regression)
rf_model1 <- randomForest(TotalPrice ~ OPCO + Product + Substrate +
                            qtyOrdered + UnitPrice + Class, 
                            data = trainData, 
                            ntree = 50,  # Number of trees (default is 500)
                            mtry = 3,
                            na.action = na.exclude) # call to handle missing values



# Extract the first tree from the model 
# k=1 indicates extracting first tree from forest
# lableVar labels the nodes for visibility
#getTree(rf_model1, k = 1, labelVar = TRUE)

# Create a simple decision tree (just for visualization purposes)
tree_model <- rpart(TotalPrice ~ OPCO + Product + Substrate + qtyOrdered +
                            UnitPrice + Class, 
                            data = trainData)

# Plot the decision tree
rpart.plot(tree_model, main = "Decision Tree Visualization")


#library(partykit)

# Train a decision tree model using partykit
#party_tree <- ctree(TotalPrice ~ OPCO + Product + Substrate + RequestedDeliveryDate + 
#                      qtyOrdered + UnitPrice + Class, 
#                      data = trainData)

# Plot the tree
#plot(party_tree, main = "Single Decision Tree from Random Forest")
```

## Model Evaluation

Both models were evaluated using the test dataset. Feature importance was examined to determine which predictors contributed most to predicting price. Plotting the importance was helpful in confirming that Model 2 relied more heavily on **qtyOrdered** and **UnitPrice** for accurate predictions, and it better incorporated factors like **Product, Substrate**, and **RequestedDeliveryDate** to refine predictions. The OOB Error estimates for the two models were very similar at 97.56% and 97.61%. Both models indicated that they can explain nearly all the variance in TotalPrice based on the features provided. The low squared residuals indicate that predictions are closer to the actual values of **TotalPrice**. The heatmap of proximity matrix showed dark colors indicating higher proximity, meaning that those two observations were often classified into the same leaf node across the trees in the random forest. 

## Statistical Programming

Data manipulation, analysis and the Random Forest algorithm were run using R version 4.4.1 (2024-06-14) in RStudio "Cranberry Hibiscus" Release for macOS. The base dataset was loaded in Rstudio using read_csv() package that takes in comma-separated files which is the format SSI data was made available. Characteristics of the data are summarized using counts and percent for continuous variables and mean and standard deviation for categorical variables. **TotalPrice** was modeled using the randomForest method using OPCO, Product, Substrate, RequestedDeliveryDate, qtyOrdered, UnitPrice and Class as predictors. 

Several R packages were installed, including the randomForest package available within R. Some of the packages and their libraries and functions are described below.

**•	dplyr:** A fast, consistent tool for working with data frame like objects, both in memory and out   of memory.

**•	knitr:** This package implements a programming paradigm that intermingles code chunks (for          computing) with prose (for documentation) in the same document. When compiling the document,      this package is responsible for executing the code chunks, and the results from computing (text   or graphics) are automatically written to the output along with the prose.

**•	ggplot2:** R package for producing visualizations of data. Unlike many graphics packages, ggplot2   uses a conceptual framework based on the grammar of graphics. 

**•	treemap:** A treemap is a space-filling visualization of hierarchical structures. This package      offers great flexibility to draw treemaps and was used to display a treemap of Total Sales by     Product.

**•	caret:** Short for Classification And REgression Training, this package contains functions to       streamline the model training process for complex regression and classification problems.

**•	randomForest:** This package contains the algorithm that implements Breiman's random forest         algorithm (based on Breiman and Cutler's original Fortran code) for classification and            regression.

**•	createDataPartition:** This is a function from the caret package that was used to partition or      split the data for training and testing.

**•	importance:** This is a extractor function for variable importance measures as produced by the      randomForest algorithm.

**•	predict:** This is a generic function in R to run predictions from the results of various model     fitting functions. The model was the randomForest model in this case and using the trained data,   a new test data set was used to make predictions.


