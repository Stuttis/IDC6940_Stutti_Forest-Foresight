---
title: "Random Forest Algorithm"
subtitle: "Methods"
author: "Stutti (Advisor: Dr. Seals)"
date: last-modified
format:
  revealjs
course: Capstone Projects in Data Science
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  echo: false
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

## Methods - Random Forest (Regression) {.smaller}

**â€¢	Summary**

    - Supervised Learning algorithm building multiple decision trees outputing the mean of individual trees.
  
    - Average of mean from multiple decision trees that make up a forest.
  
    - Reduces overfitting, de-emphasizes irrelevant features and works well with large data sets.
    
**â€¢	Method - Components**

    - Bootstrap Sampling: Each tree is trained using random sampling with replacement from the training data (80% of original dataset). Remaining data (not in the sampled data) is called Out-of-Bag (OOB) data and used to test model accuracy.
    
    - Random Feature Selection: 3 randomly selected features are considered at each split in the trees. This reduces correlation between trees and improves generalization.
    
    - Bagging: This is the aggregation or final prediction (average) from all trees in the forest.
    
## Method - Parameters {.smaller}

The dependent variable, *TotalPrice*, was modeled using the randomForest() function in R. The model was trained to predict Sales prices on orders based on the following independent variables (predictors): *OPCO, Product, Substrate, Requested Delivery, Quantity Ordered, Unit Price and Class*.


  **â€¢	trainData:** The model was trained on trainData (80% data split) that contains the target variable and all predictors.
  
  **â€¢	ntree:** The number of trees to build is determined by `ntree`. Two models were tested, one with 100 trees and second with 300.
  
  **â€¢	mtry:** 3 random features selected for consideration at each split.
  
  **â€¢	importance:** This function computed each feature's influence on the outcome.
  
  **â€¢	proximity:** This function computed closeness of data points useful to identify relationships in the data.
  
## Methods - Mathematical Concepts {.scrollable}

**Below are the underlying equations involved in this Random Forest:**

  1. `Mean Squared Error (MSE)`: For a continuous target variable, *Total Price*, the criteria for splitting at each node is based on minimizing the `(MSE)`:

      $$ MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y})^2 $$
where:

â€¢	ğ‘= number of data points in the node

â€¢	ğ‘¦ğ‘–= actual value of the target variable for the ğ‘–-th data point

â€¢	ğ‘¦^ğ‘–= predicted value (average of the target variable in the node)

  2. `OOB Error` is an internal estimate of the model's performance. The OOB MSE is calculated as:
  
      $$ OOB\_ MSE = \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}OOB,i)^2 $$
where ğ‘¦^ğ‘‚ğ‘‚B,ğ‘– is the average of the predictions for theğ‘–-th data point across all trees where that point was not included in the bootstrap sample.
  
  
## Model Comparison {.smaller}

**Model 1 vs Model 2**

    â€¢	100 trees vs 300 trees 
  
    â€¢	The choice of 300 trees provided slightly better results. However, the difference in performance is minor.
    
    â€¢	Considering the computational cost of training more trees:
    
          1. Choose **Model 1** if speed or resource efficiency is more important.
        
          2. Choose **Model 2** if accuracy is critical. 
        
        
